{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "from pretrain import EmberMLP\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "\n",
    "def set_seed(seed_value=1763):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    logging.debug(f\" [*] {time.ctime()}: Using random seed for all libraries: {seed_value}\")\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "EMBER_FEATURE_DIM = 2381\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "RANDOM_SEED = 1991\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training w/ Ember features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(r\"vectorize_output_1657871489\\X_ember_trainset.npy\")\n",
    "y_train = np.load(r\"vectorize_output_1657871489\\y_ember_trainset.npy\")\n",
    "X_val = np.load(r\"vectorize_output_1657871489\\X_ember_valset.npy\").astype(np.int8)\n",
    "y_val = np.load(r\"vectorize_output_1657871489\\y_ember_valset.npy\").astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 2381)\n",
      "(31, 2381)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[295754.8125],\n",
       "        [ 39226.1250],\n",
       "        [ 51071.0000],\n",
       "        [ 81800.4609]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmberMLP()\n",
    "net(torch.Tensor(X_train))[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmberMLP()\n",
    "rr = net.get_representations(torch.Tensor(X_train))\n",
    "rr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, loss_function, epoch_id, verbosity_batches=100):\n",
    "    model.train()\n",
    "\n",
    "    train_metrics = []\n",
    "    train_loss = []\n",
    "    now = time.time()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device).reshape(-1,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss = loss_function(logits, target)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward() # derivatives\n",
    "        optimizer.step() # parameter update\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        \n",
    "        accuracy = (preds == target).cpu().numpy().mean() * 100\n",
    "        f1 = f1_score(target, preds)\n",
    "        rocauc = roc_auc_score(target, preds)\n",
    "        train_metrics.append([accuracy, f1, rocauc])\n",
    "        \n",
    "        if batch_idx % verbosity_batches == 0:\n",
    "            logging.warning(\" [*] {}: Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f} | Elapsed: {:.2f}s\".format(\n",
    "                time.ctime(), epoch_id, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), np.mean([x[0] for x in train_metrics]), time.time()-now))\n",
    "            now = time.time()\n",
    "\n",
    "    return train_loss, np.array(train_metrics).mean(axis=0).reshape(-1,3), logits, target\n",
    "\n",
    "def evaluate(model, device, val_loader, loss_function):\n",
    "    model.eval()\n",
    "\n",
    "    val_metrics = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).reshape(-1,1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(data)\n",
    "        \n",
    "        loss = loss_function(logits, target)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == target).cpu().numpy().mean() * 100\n",
    "        f1 = f1_score(target, preds)\n",
    "        roc_auc = roc_auc_score(target, preds)\n",
    "        val_metrics.append([accuracy, f1, roc_auc])\n",
    "        \n",
    "    return val_loss, np.array(val_metrics).mean(axis=0).reshape(-1,3)\n",
    "\n",
    "def dump_results(model, train_losses, train_metrics, val_losses, val_metrics, duration, args, epoch):\n",
    "    prefix = f\"ep{epoch}-optim_{args.optimizer}-lr{args.learning_rate}-l2reg{args.l2}-dr{args.dropout}\"\n",
    "    prefix += f\"_arr-ed{args.embedding_dim}-kb{args.keep_bytes}-pl{args.padding_length}\"\n",
    "    prefix += f\"_model-conv{args.num_filters}-bn_c{args.batch_norm_conv}_f{args.batch_norm_ffnn}-ffnn{'_'.join([str(x) for x in args.hidden_layers])}\"\n",
    "    \n",
    "    model_file = f\"{prefix}-model.torch\"\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "    with open(f\"{prefix}-train_losses.pickle\", \"wb\") as f:\n",
    "        pickle.dump(train_losses, f)\n",
    "    \n",
    "    with open(f\"{prefix}-val_losses.pickle\", \"wb\") as f:\n",
    "        pickle.dump(val_losses, f)\n",
    "    \n",
    "    # in form [train_acc, train_f1]\n",
    "    np.save(f\"{prefix}-train_metrics.pickle\", train_metrics)\n",
    "\n",
    "    # in form [val_acc, val_f1]\n",
    "    np.save(f\"{prefix}-val_metrics.pickle\", val_metrics)\n",
    "\n",
    "    with open(f\"{prefix}-duration.pickle\", \"wb\") as f:\n",
    "        pickle.dump(duration, f)\n",
    "\n",
    "    dumpstring = f\"\"\"\n",
    "     [!] {time.ctime()}: Dumped results:\n",
    "            model: {model_file}\n",
    "            train loss list: {prefix}-train_losses.pickle\n",
    "            validation loss list: {prefix}-val_losses.pickle\n",
    "            train metrics : {prefix}-train_metrics.pickle\n",
    "            validation metrics : {prefix}-train_metrics.pickle\n",
    "            duration: {prefix}-duration.pickle\"\"\"\n",
    "    logging.warning(dumpstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.8264e-02, -9.3230e-03,  1.6173e-02,  ..., -1.1918e-02,\n",
       "          3.2062e-03,  4.9199e-03],\n",
       "        [-4.5862e-03,  1.1049e-02, -1.4949e-02,  ...,  2.0208e-02,\n",
       "          2.5320e-03, -3.8756e-04],\n",
       "        [ 1.4658e-02,  1.3672e-02, -3.7788e-03,  ...,  1.4896e-02,\n",
       "          3.5919e-03, -9.2345e-03],\n",
       "        ...,\n",
       "        [ 5.9809e-03,  6.0326e-04, -6.9606e-04,  ..., -1.8532e-02,\n",
       "         -9.8626e-03,  1.2406e-02],\n",
       "        [-1.1200e-02,  5.7746e-03,  1.9879e-02,  ...,  1.5798e-02,\n",
       "          8.7864e-03, -5.8316e-03],\n",
       "        [-1.6916e-02,  1.4591e-02,  7.5539e-03,  ...,  1.9885e-03,\n",
       "         -2.0210e-05,  1.9130e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmberMLP()\n",
    "aa = [x for x in model.parameters()]\n",
    "aa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Started epoch: 1\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Train Epoch: 1 [0/74 (0%)]\tLoss: -0.000000\tAcc: 16.22 | Elapsed: 0.05s\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022:    1    | Tr.loss:   0.000000   | Tr.acc.:   16.22   | Val.loss:  0.000000  | Val.acc.:   9.68    |   0.06   \n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Started epoch: 2\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Train Epoch: 2 [0/74 (0%)]\tLoss: -0.000000\tAcc: 16.22 | Elapsed: 0.06s\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022:    2    | Tr.loss:   0.000000   | Tr.acc.:   16.22   | Val.loss:  0.000000  | Val.acc.:   9.68    |   0.07   \n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Started epoch: 3\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Train Epoch: 3 [0/74 (0%)]\tLoss: -0.000000\tAcc: 16.22 | Elapsed: 0.04s\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022:    3    | Tr.loss:   0.000000   | Tr.acc.:   16.22   | Val.loss:  0.000000  | Val.acc.:   9.68    |   0.05   \n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Started epoch: 4\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022: Train Epoch: 4 [0/74 (0%)]\tLoss: -0.000000\tAcc: 16.22 | Elapsed: 0.03s\n",
      "WARNING:root: [*] Fri Jul 15 16:08:38 2022:    4    | Tr.loss:   0.000000   | Tr.acc.:   16.22   | Val.loss:  0.000000  | Val.acc.:   9.68    |   0.04   \n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),\n",
    "    batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)),\n",
    "    batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = EmberMLP()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "train_metrics = []\n",
    "val_losses = []\n",
    "val_metrics = []\n",
    "duration = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, 5):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        logging.warning(f\" [*] {time.ctime()}: Started epoch: {epoch}\")\n",
    "\n",
    "        train_loss, train_m, logits, target = train(model, device, train_loader, optimizer, loss_function, epoch, 100)\n",
    "        train_losses.extend(train_loss)\n",
    "        train_metrics.append(train_m)\n",
    "\n",
    "        # After the completion of each training epoch, measure the model's performance on validation set.\n",
    "        val_loss, val_m = evaluate(model, device, val_loader, loss_function)\n",
    "        val_losses.extend(val_loss)\n",
    "        val_metrics.append(val_m)\n",
    "\n",
    "        # Print performance over the entire training data\n",
    "        time_elapsed = time.time() - epoch_start_time\n",
    "        duration.append(time_elapsed)\n",
    "        logging.warning(f\" [*] {time.ctime()}: {epoch:^7} | Tr.loss: {np.mean(train_loss):^12.6f} | Tr.acc.: {np.mean([x[0] for x in train_m]):^9.2f} | Val.loss: {np.mean(val_loss):^10.6f} | Val.acc.: {np.mean([x[0] for x in val_m]):^9.2f} | {time_elapsed:^9.2f}\")\n",
    "    #dump_results(net, train_losses, np.vstack(train_metrics), val_losses, np.vstack(val_metrics), duration, args, epoch)\n",
    "except KeyboardInterrupt as ex:\n",
    "    print(\"interrupted\")\n",
    "    #dump_results(net, train_losses, train_metrics, val_losses, val_metrics, duration, args, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.7475e-02, -8.9198e-03,  1.5474e-02,  ..., -1.1403e-02,\n",
       "          3.0675e-03,  4.7071e-03],\n",
       "        [-4.3879e-03,  1.0571e-02, -1.4303e-02,  ...,  1.9334e-02,\n",
       "          2.4225e-03, -3.7081e-04],\n",
       "        [ 1.4024e-02,  1.3080e-02, -3.6154e-03,  ...,  1.4252e-02,\n",
       "          3.4365e-03, -8.8352e-03],\n",
       "        ...,\n",
       "        [ 5.7223e-03,  5.7717e-04, -6.6597e-04,  ..., -1.7730e-02,\n",
       "         -9.4362e-03,  1.1870e-02],\n",
       "        [-1.0715e-02,  5.5249e-03,  1.9020e-02,  ...,  1.5115e-02,\n",
       "          8.4065e-03, -5.5794e-03],\n",
       "        [-1.6184e-02,  1.3960e-02,  7.2272e-03,  ...,  1.9025e-03,\n",
       "         -1.9336e-05,  1.8303e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = [x for x in model.parameters()]\n",
    "print(aa == bb)\n",
    "bb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('_localQuoVadisPythonEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9535e8dccbd70afdbee2a03d6200a267b05376dbac5f7bee202056aa5f9c894"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
