{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from preprocessing.text import normalize_path, load_csv\n",
    "from preprocessing.array import fix_length, byte_filter, remap\n",
    "\n",
    "df = load_csv(\"data/bohacek_20211022113102.csv\") # ~7.5s\n",
    "df.x = df.x.apply(normalize_path) # 5s\n",
    "df.x = df.x.str.encode(\"utf-8\", \"ignore\").apply(lambda x: np.array(list(x), dtype=int)) # 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter([x for y in df.x.values for x in y]) # 4s\n",
    "keep_bytes = [x[0] for x in counter.most_common(150)] # 0.5s\n",
    "\n",
    "df.x = df.x.apply(fix_length) # 7.5s\n",
    "\n",
    "X_ti = np.stack(df.x.values)\n",
    "X_ti = byte_filter(X_ti, keep_bytes+[0]) # 3.5s\n",
    "y_ti = np.stack(df.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading clean Win 10 system files too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/win10_fullfilesystem.txt\") as f:\n",
    "    win10files = f.readlines()\n",
    "\n",
    "win10files = pd.read_csv(\"data/win10_fullfilesystem.txt\", header=None) # 0.5s\n",
    "win10files.columns = [\"x\"]\n",
    "win10files = win10files.x.apply(normalize_path) # 2s\n",
    "win10files = win10files.str.encode(\"utf-8\", \"ignore\").apply(lambda x: np.array(list(x), dtype=int)) # 2s\n",
    "win10files = win10files.apply(fix_length) # 2s\n",
    "\n",
    "X_win10files = np.stack(win10files.values)\n",
    "X_win10files = byte_filter(X_win10files, keep_bytes+[0]) # 3.5s\n",
    "y_win10files = np.zeros(X_win10files.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking all data together and remapping UTF-8 bytes to sequential 0-152 integers (needed from `nn.Embedding`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TI data: 141158 benign, 125120 malicious\n",
      "Windows 10 data: 122410 benign\n",
      "Total: Benign 67.81 %, Malicious 32.19 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"TI data: {y_ti[y_ti==0].shape[0]} benign, {y_ti[y_ti==1].shape[0]} malicious\")\n",
    "print(f\"Windows 10 data: {y_win10files.shape[0]} benign\")\n",
    "\n",
    "X = np.vstack([X_ti, X_win10files])\n",
    "# remapping for embedding: ~ 5 s\n",
    "X = remap(X, keep_bytes)\n",
    "\n",
    "y = np.vstack([y_ti.reshape(-1,1), y_win10files.reshape(-1,1)]).squeeze()\n",
    "\n",
    "print(f\"Total: Benign {y[y==0].shape[0]*100/y.shape[0]:.2f} %, Malicious {y[y==1].shape[0]*100/y.shape[0]:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "class Model_1st(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size = 152, \n",
    "                embedding_dim = 32,\n",
    "                filter_sizes = [2, 3, 4, 5],\n",
    "                num_filters = [128, 128, 128, 128],\n",
    "                num_classes = 2,\n",
    "                dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # embdding\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                  embedding_dim, \n",
    "                                  padding_idx=0)\n",
    "        \n",
    "        # convolutions\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "                            nn.Conv1d(in_channels=embedding_dim,\n",
    "                                out_channels=num_filters[i],\n",
    "                                kernel_size=filter_sizes[i])\n",
    "                            for i in range(len(filter_sizes))\n",
    "                            ])\n",
    "\n",
    "        # Fully-connected layers and Dropout\n",
    "        self.fc_hidden = nn.Linear(np.sum(num_filters), 128)\n",
    "        self.fc_output = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Non-linearities\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def conv_and_max_pool(x, conv):\n",
    "        \"\"\"Convolution and global max pooling layer\"\"\"\n",
    "        return F.relu(conv(x).permute(0, 2, 1).max(1)[0])\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get embeddings from `x`. \n",
    "        # Output shape: (b, max_len, embed_dim), \n",
    "        # torch.Size([1024, 150, 32])\n",
    "        embedded = self.embedding(inputs).permute(0, 2, 1)\n",
    "        # .permute() to change sequence of max_len and embed_dim, so shape is:\n",
    "        # torch.Size([1024, 32, 150])\n",
    "        # needed for Conv1D\n",
    "        \n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv = [self.conv_and_max_pool(embedded, conv1d) for conv1d in self.conv1d_list]\n",
    "        \n",
    "        # USED IN PAPER SOMETHING LIKE THIS?\n",
    "        #x_norm_list = [nn.LayerNorm(x.shape)(x) for x in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = self.dropout(torch.cat(x_conv, dim=1))\n",
    "        x_h = self.relu(self.fc_hidden(x_fc))\n",
    "        out = self.fc_output(x_h)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/310950 (0%)]\tLoss: 0.955297\tAccuracy: 31.54\n",
      "Train Epoch: 1 [102400/310950 (33%)]\tLoss: 0.559228\tAccuracy: 68.05\n",
      "Train Epoch: 1 [204800/310950 (66%)]\tLoss: 0.525549\tAccuracy: 70.26\n",
      "Train Epoch: 1 [307200/310950 (99%)]\tLoss: 0.466668\tAccuracy: 71.99\n",
      "   2    |   0.539600   |   72.03   |  0.423571  |   79.67   |  442.77  \n",
      "Train Epoch: 2 [0/310950 (0%)]\tLoss: 0.471404\tAccuracy: 75.88\n",
      "Train Epoch: 2 [102400/310950 (33%)]\tLoss: 0.480166\tAccuracy: 76.66\n",
      "Train Epoch: 2 [204800/310950 (66%)]\tLoss: 0.411170\tAccuracy: 77.11\n",
      "Train Epoch: 2 [307200/310950 (99%)]\tLoss: 0.431142\tAccuracy: 77.44\n",
      "   3    |   0.450021   |   77.45   |  0.382166  |   81.16   |  421.19  \n",
      "Train Epoch: 3 [0/310950 (0%)]\tLoss: 0.402938\tAccuracy: 79.98\n",
      "Train Epoch: 3 [102400/310950 (33%)]\tLoss: 0.403887\tAccuracy: 78.98\n",
      "Train Epoch: 3 [204800/310950 (66%)]\tLoss: 0.425321\tAccuracy: 79.16\n",
      "Train Epoch: 3 [307200/310950 (99%)]\tLoss: 0.419037\tAccuracy: 79.41\n",
      "   4    |   0.415310   |   79.42   |  0.343584  |   83.17   |  417.78  \n",
      "Train Epoch: 4 [0/310950 (0%)]\tLoss: 0.408485\tAccuracy: 79.69\n",
      "Train Epoch: 4 [102400/310950 (33%)]\tLoss: 0.410987\tAccuracy: 80.09\n",
      "Train Epoch: 4 [204800/310950 (66%)]\tLoss: 0.381953\tAccuracy: 80.28\n",
      "Train Epoch: 4 [307200/310950 (99%)]\tLoss: 0.373361\tAccuracy: 80.47\n",
      "   5    |   0.395659   |   80.47   |  0.323936  |   85.18   |  418.02  \n"
     ]
    }
   ],
   "source": [
    "# ====== ENSURING REPRODUCIBILITY =======\n",
    "SEED = 1763\n",
    "set_seed(seed_value=SEED)\n",
    "\n",
    "# ====== DATA PREPARATION =======\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.LongTensor(X_train),torch.LongTensor(y_train)),\n",
    "    batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.LongTensor(X_val),torch.LongTensor(y_val)),\n",
    "    batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ====== MODEL & TRAINING ENVIRONMENT DEFINITION =========\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "VOCAB = len(keep_bytes) + 2 # + 2 since: 0 - pad, 1 - rare byte\n",
    "EPOCHS = 4\n",
    "device = \"cpu\"\n",
    "\n",
    "model = Model_1st(vocab_size=VOCAB, embedding_dim=EMBEDDING_DIM).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======== TRAINING & EVAL FUNCTIONS ===========\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss = criterion(logits, target)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward() # derivatives\n",
    "        optimizer.step() # parameter update\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = (preds == target).cpu().numpy().mean() * 100\n",
    "        train_accuracy.append(accuracy)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.2f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), np.mean(train_accuracy)))\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, device, val_loader):\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(data)\n",
    "        \n",
    "        loss = criterion(logits, target)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == target).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "# ======== ACTUAL TRAINING ===========\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0_epoch = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch) # ~140s per 1024 samples\n",
    "    train_losses.extend(train_loss)\n",
    "\n",
    "    if val_loader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_acc = evaluate(model, device, val_loader)\n",
    "            val_losses.extend(val_loss)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch + 1:^7} | {np.mean(train_loss):^12.6f} | {np.mean(train_acc):^9.2f} | {np.mean(val_loss):^10.6f} | {np.mean(val_acc):^9.2f} | {time_elapsed:^9.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/310950 (0%)]\tLoss: 0.389952\tAccuracy: 79.59\n",
      "Train Epoch: 6 [102400/310950 (33%)]\tLoss: 0.386662\tAccuracy: 81.43\n",
      "Train Epoch: 6 [204800/310950 (66%)]\tLoss: 0.360878\tAccuracy: 81.51\n",
      "Train Epoch: 6 [307200/310950 (99%)]\tLoss: 0.351639\tAccuracy: 81.53\n",
      "   7    |   0.378999   |   81.54   |  0.313622  |   85.46   |  425.63  \n",
      "Train Epoch: 7 [0/310950 (0%)]\tLoss: 0.372812\tAccuracy: 82.32\n",
      "Train Epoch: 7 [102400/310950 (33%)]\tLoss: 0.357280\tAccuracy: 81.76\n",
      "Train Epoch: 7 [204800/310950 (66%)]\tLoss: 0.362666\tAccuracy: 81.97\n",
      "Train Epoch: 7 [307200/310950 (99%)]\tLoss: 0.362496\tAccuracy: 82.11\n",
      "   8    |   0.367778   |   82.12   |  0.302396  |   86.21   |  419.92  \n",
      "Train Epoch: 8 [0/310950 (0%)]\tLoss: 0.337637\tAccuracy: 82.62\n",
      "Train Epoch: 8 [102400/310950 (33%)]\tLoss: 0.362778\tAccuracy: 82.65\n",
      "Train Epoch: 8 [204800/310950 (66%)]\tLoss: 0.368646\tAccuracy: 82.60\n",
      "Train Epoch: 8 [307200/310950 (99%)]\tLoss: 0.349128\tAccuracy: 82.66\n",
      "   9    |   0.358282   |   82.67   |  0.306084  |   85.25   |  431.39  \n",
      "Train Epoch: 9 [0/310950 (0%)]\tLoss: 0.358299\tAccuracy: 81.45\n",
      "Train Epoch: 9 [102400/310950 (33%)]\tLoss: 0.331980\tAccuracy: 83.02\n",
      "Train Epoch: 9 [204800/310950 (66%)]\tLoss: 0.353824\tAccuracy: 83.11\n",
      "Train Epoch: 9 [307200/310950 (99%)]\tLoss: 0.318959\tAccuracy: 83.17\n",
      "  10    |   0.349409   |   83.17   |  0.289382  |   86.77   |  427.62  \n",
      "Train Epoch: 10 [0/310950 (0%)]\tLoss: 0.372096\tAccuracy: 81.35\n",
      "Train Epoch: 10 [102400/310950 (33%)]\tLoss: 0.317670\tAccuracy: 83.53\n",
      "Train Epoch: 10 [204800/310950 (66%)]\tLoss: 0.309333\tAccuracy: 83.60\n",
      "Train Epoch: 10 [307200/310950 (99%)]\tLoss: 0.331051\tAccuracy: 83.62\n",
      "  11    |   0.341794   |   83.61   |  0.282026  |   87.27   |  426.21  \n",
      "Train Epoch: 11 [0/310950 (0%)]\tLoss: 0.322856\tAccuracy: 85.06\n",
      "Train Epoch: 11 [102400/310950 (33%)]\tLoss: 0.298940\tAccuracy: 83.86\n",
      "Train Epoch: 11 [204800/310950 (66%)]\tLoss: 0.329438\tAccuracy: 83.92\n",
      "Train Epoch: 11 [307200/310950 (99%)]\tLoss: 0.380561\tAccuracy: 83.99\n",
      "  12    |   0.335450   |   83.99   |  0.280148  |   87.04   |  423.52  \n",
      "Train Epoch: 12 [0/310950 (0%)]\tLoss: 0.319716\tAccuracy: 84.86\n",
      "Train Epoch: 12 [102400/310950 (33%)]\tLoss: 0.316534\tAccuracy: 84.23\n",
      "Train Epoch: 12 [204800/310950 (66%)]\tLoss: 0.349126\tAccuracy: 84.25\n",
      "Train Epoch: 12 [307200/310950 (99%)]\tLoss: 0.324459\tAccuracy: 84.30\n",
      "  13    |   0.329883   |   84.30   |  0.274923  |   87.34   |  426.82  \n",
      "Train Epoch: 13 [0/310950 (0%)]\tLoss: 0.328890\tAccuracy: 84.96\n",
      "Train Epoch: 13 [102400/310950 (33%)]\tLoss: 0.348590\tAccuracy: 84.51\n",
      "Train Epoch: 13 [204800/310950 (66%)]\tLoss: 0.329974\tAccuracy: 84.58\n",
      "Train Epoch: 13 [307200/310950 (99%)]\tLoss: 0.343349\tAccuracy: 84.74\n",
      "  14    |   0.323190   |   84.73   |  0.268977  |   87.89   |  416.89  \n",
      "Train Epoch: 14 [0/310950 (0%)]\tLoss: 0.322600\tAccuracy: 84.47\n",
      "Train Epoch: 14 [102400/310950 (33%)]\tLoss: 0.315641\tAccuracy: 84.77\n",
      "Train Epoch: 14 [204800/310950 (66%)]\tLoss: 0.322905\tAccuracy: 84.88\n",
      "Train Epoch: 14 [307200/310950 (99%)]\tLoss: 0.313828\tAccuracy: 84.89\n",
      "  15    |   0.318778   |   84.90   |  0.265323  |   88.13   |  425.85  \n",
      "Train Epoch: 15 [0/310950 (0%)]\tLoss: 0.313237\tAccuracy: 84.77\n",
      "Train Epoch: 15 [102400/310950 (33%)]\tLoss: 0.325612\tAccuracy: 84.94\n",
      "Train Epoch: 15 [204800/310950 (66%)]\tLoss: 0.316705\tAccuracy: 85.12\n",
      "Train Epoch: 15 [307200/310950 (99%)]\tLoss: 0.287566\tAccuracy: 85.20\n",
      "  16    |   0.313251   |   85.20   |  0.261025  |   88.23   |  432.68  \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "for epoch in range(6, EPOCHS + 1):\n",
    "    t0_epoch = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch) # ~140s per 1024 samples\n",
    "    train_losses.extend(train_loss)\n",
    "\n",
    "    if val_loader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_acc = evaluate(model, device, val_loader)\n",
    "            val_losses.extend(val_loss)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch + 1:^7} | {np.mean(train_loss):^12.6f} | {np.mean(train_acc):^9.2f} | {np.mean(val_loss):^10.6f} | {np.mean(val_acc):^9.2f} | {time_elapsed:^9.2f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34c873defd3a8992f50d66eecf6d87e388d7227e176883a0e053f6f337feb4ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
