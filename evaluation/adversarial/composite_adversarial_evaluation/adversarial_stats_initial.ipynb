{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import time\n",
    "repo_root = \"/data/quo.vadis/\"\n",
    "sys.path.append(repo_root)\n",
    "from models import CompositeClassifier\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "EMBER_THRESHOLD = 0.8336\n",
    "\n",
    "X_TRAIN = np.load(repo_root+\"evaluation/composite/X-1647041985-early-fusion-vectors-train.arr\")\n",
    "Y_TRAIN = np.load(repo_root+\"evaluation/composite/y-1647041985-train.arr\")\n",
    "\n",
    "X_VAL = np.load(repo_root+\"evaluation/composite/X-1647097165-early-fusion-vectors-val.arr\")\n",
    "Y_VAL = np.load(repo_root+\"evaluation/composite/y-1647097165-val.arr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite preprocessing of adversarial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adversarial_samples(folder):\n",
    "    fullpaths = [repo_root+folder+x for x in os.listdir(repo_root+folder)]\n",
    "    adversarial_samples = [x for x in fullpaths if not os.path.islink(x)]\n",
    "    adversarial_samples.sort()\n",
    "    return adversarial_samples\n",
    "\n",
    "ADVERSARIAL_EMULATED_SET_FOLDER = [\"data/adversarial.emulated/partial_reports_ember_15sections_10population\"]#, \"data/adversarial.emulated/reports_ember_10sections_10population/\", \"data/adversarial.emulated/reports_ember_5sections_10population/\"]\n",
    "ADVERSARIAL_RAW_SET_FOLDER = [\"data/adversarial.samples/samples_adversarial_testset_gamma_ember_15sections_10population/\"]#, \"data/adversarial.samples/samples_adversarial_testset_gamma_ember_sections/10/\", \"data/adversarial.samples/samples_adversarial_testset_gamma_ember_sections/5/\"]\n",
    "ADV_SAMPLES = [get_adversarial_samples(x) for x in ADVERSARIAL_RAW_SET_FOLDER]\n",
    "ADV_SAMPLE_HASHES = [[x.split(\"/\")[-1] for x in y] for y in ADV_SAMPLES]\n",
    "\n",
    "ARRAY_FOLDER  = [\"evaluation/adversarial/composite_adversarial_evaluation/arrays_ember_15sections_10population/\"]#, \"evaluation/adversarial/composite_adversarial_evaluation/arrays_ember_10sections_10population/\", \"evaluation/adversarial/composite_adversarial_evaluation/arrays_ember_5sections_10population/\"]\n",
    "ARRAY_FOLDER = [repo_root + x for x in ARRAY_FOLDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8896]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in ADV_SAMPLE_HASHES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: there is less total samples because we skipped benignware and malware that was already evasive!\n",
    "\n",
    "Total file number match with filesystem data:\n",
    "```\n",
    "/data/quo.vadis/adversarial/samples_adversarial_testset_gamma_ember]$ find . -type f | wc -l\n",
    "8896\n",
    "```\n",
    "\n",
    "Adversarial length match with report_db in emulation module - it loads reports from `data/adversarial.emulation.dataset/reports_ember` since passed as parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5399"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = {}\n",
    "x_adv_ember = {}\n",
    "x_orig_ember = {}\n",
    "y_ember_orig = {}\n",
    "y_ember_adv = {}\n",
    "y_ember_orig_int = {}\n",
    "y_ember_adv_int = {}\n",
    "\n",
    "idx_map = {0:\"15\", 1:\"10\", 2:\"5\"}\n",
    "for i in range(1):\n",
    "    sections = idx_map[i]\n",
    "    \n",
    "    classifiers[sections] = CompositeClassifier(root=repo_root,\n",
    "                            emulation_report_path=ADVERSARIAL_EMULATED_SET_FOLDER[i],\n",
    "                            rawpe_db_path=ADVERSARIAL_RAW_SET_FOLDER[i])\n",
    "    \n",
    "    x_adv_ember[sections] = np.load(ARRAY_FOLDER[i] + \"X-gamma-vs-ember-early-fusion-pass-only-adv.arr\") \n",
    "    x_orig_ember[sections] = np.load(ARRAY_FOLDER[i] + \"X-gamma-vs-ember-early-fusion-pass-orig-only-adv.arr\") \n",
    "\n",
    "    y_ember_adv[sections] = np.load(ARRAY_FOLDER[i] + \"y-gamma-vs-ember-scores-only-adv.arr\")\n",
    "    y_ember_orig[sections] = np.load(ARRAY_FOLDER[i] + \"y-gamma-vs-ember-scores-orig-only-adv.arr\")\n",
    "    \n",
    "    y_ember_adv_int[sections] = (y_ember_adv[sections] > EMBER_THRESHOLD).astype(int)\n",
    "    y_ember_orig_int[sections] = (y_ember_orig[sections] > EMBER_THRESHOLD).astype(int)\n",
    "\n",
    "len(classifiers[\"15\"].modules[\"emulation\"].report_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(models, x_trains, y_train, save=False):\n",
    "    for model in models:\n",
    "        print(f\"training late fusion model for {model}...\")\n",
    "        now = time.time()\n",
    "        models[model].fit(x_trains[model], y_train)\n",
    "        print(f\"training done for {model}... took: {time.time()-now:.2f}s\")\n",
    "        if save:\n",
    "            os.makedirs(\"late_fusion_model_fit\", exist_ok=True)\n",
    "            models[model].save_late_fusion_model(filename=\"late_fusion_model_fit/\"+model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to modify report path to original dataset\n",
    "def get_models_and_correct_early_fusion_arrays(x_adv_ember, y_ember_adv, x_orig_ember, y_ember_orig):\n",
    "    modulelist = [[\"filepaths\"], [\"emulation\"], [\"ember\"],\n",
    "                [\"ember\", \"emulation\"], [\"ember\", \"filepaths\", \"emulation\"]]\n",
    "    models = {}\n",
    "    x_trains = {}\n",
    "    #x_vals = {}\n",
    "    x_earlyfusion_adv = {}\n",
    "    x_earlyfusion_orig = {}\n",
    "\n",
    "    for modules in modulelist:\n",
    "        name = \"_\".join(modules)\n",
    "        if len(modules) == 4:\n",
    "            name = \"all\"\n",
    "\n",
    "        models[name] = CompositeClassifier(modules=modules, root=repo_root)\n",
    "        x_trains[name] = models[name].get_modular_x(modules, X_TRAIN)\n",
    "        #x_vals[name] = models[name].get_modular_x(modules, X_VAL)\n",
    "        \n",
    "        x_earlyfusion_adv[name] = models[name].get_modular_x(modules, x_adv_ember)\n",
    "        x_earlyfusion_orig[name] = models[name].get_modular_x(modules, x_orig_ember)\n",
    "        \n",
    "        if \"ember\" in modules:\n",
    "            ember_index = modules.index(\"ember\")\n",
    "            # replace ember column with y pass\n",
    "            x_earlyfusion_adv[name][:,ember_index] = y_ember_adv\n",
    "            x_earlyfusion_orig[name][:,ember_index] = y_ember_orig\n",
    "\n",
    "    # Remember: this .fit() really trains only late fusion model\n",
    "    models = fit(models, x_trains, Y_TRAIN, save=False)\n",
    "    \n",
    "    return models, x_earlyfusion_adv, x_earlyfusion_orig\n",
    "\n",
    "\n",
    "def get_metrics_adv_nonadv(model, x_val, y_val, x_adv, y_adv):\n",
    "    probs = model.predict_proba(x_val)[:,1]\n",
    "    probs_adv = model.predict_proba(x_adv)[:,1]\n",
    "    \n",
    "    preds = np.where(probs > 0.5, 1, 0)\n",
    "    preds_adv = np.where(probs_adv > 0.5, 1, 0)\n",
    "\n",
    "    print(\"Non-Adversarial Set accuracy:\", end=\" \")\n",
    "    print(accuracy_score(y_val, preds))\n",
    "    \n",
    "    print(\"Adversarial Set accuracy:\", end=\"     \")\n",
    "    print(accuracy_score(y_adv, preds_adv))\n",
    "\n",
    "    return probs, probs_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== ember (secml) ======\n",
      "Non-Adversarial Set accuracy: 0.9814780514910169\n",
      "Adversarial Set accuracy:     0.7193924800889053\n",
      "\n",
      " ====== malconv ======\n",
      "Non-Adversarial Set accuracy: 0.9855528801629931\n",
      "Adversarial Set accuracy:     0.9803667345804778\n",
      "\n",
      " ====== filepaths ======\n",
      "Non-Adversarial Set accuracy: 0.9781441007593998\n",
      "Adversarial Set accuracy:     0.9781441007593998\n",
      "\n",
      " ====== emulation ======\n",
      "Non-Adversarial Set accuracy: 0.9955547323578441\n",
      "Adversarial Set accuracy:     0.9764771253935914\n",
      "\n",
      " ====== ember ======\n",
      "Non-Adversarial Set accuracy: 1.0\n",
      "Adversarial Set accuracy:     0.8705315799222079\n",
      "\n",
      " ====== ember_emulation ======\n",
      "Non-Adversarial Set accuracy: 0.9990739025745509\n",
      "Adversarial Set accuracy:     0.9562882015187998\n",
      "\n",
      " ====== ember_filepaths_emulation ======\n",
      "Non-Adversarial Set accuracy: 0.9887016114095203\n",
      "Adversarial Set accuracy:     0.9851824411928135\n",
      "\n",
      " ====== all ======\n",
      "Non-Adversarial Set accuracy: 0.9887016114095203\n",
      "Adversarial Set accuracy:     0.9851824411928135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_adv = np.ones(len(x_adv_ember))\n",
    "\n",
    "print(\"====== ember (secml) ======\")\n",
    "print(\"Non-Adversarial Set accuracy:\", end=\" \")\n",
    "print(accuracy_score(y_adv, y_ember_orig_int))\n",
    "print(\"Adversarial Set accuracy:\", end=\"     \")\n",
    "print(accuracy_score(y_adv, y_ember_adv_int))\n",
    "\n",
    "probbs, probbs_adv = {}, {}\n",
    "for model in models:\n",
    "    x_orig_t = x_ember_orig[model]\n",
    "    x_adv_t = x_ember_adv[model]\n",
    "    print(\"\\n\", \"=\"*6, model, \"=\"*6)\n",
    "    probbs[model], probbs_adv[model] = get_metrics_adv_nonadv(models[model], x_orig_t, y_adv, x_adv_t, y_adv)\n",
    "    # get_metrics_adv_nonadv(models[model], x_test_t, y_test, x_adv_t, y_adv) if you want against full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100: ember (secml) classifies as benign in orig malware set\n",
      " 1515: ember (secml) classifies as benign in adversarial malware set\n",
      " 1415, 26.21%: evasive samples and ratio against ember\n",
      "\n",
      " 78: malconv classifies as benign in orig malware set\n",
      " 106: malconv classifies as benign in adversarial malware set\n",
      " 28, 0.52%: evasive samples and ratio against malconv \n",
      "\n",
      " 118: filepaths classifies as benign in orig malware set\n",
      " 118: filepaths classifies as benign in adversarial malware set\n",
      " 0, 0.00%: evasive samples and ratio against filepaths \n",
      "\n",
      " 24: emulation classifies as benign in orig malware set\n",
      " 127: emulation classifies as benign in adversarial malware set\n",
      " 103, 1.91%: evasive samples and ratio against emulation \n",
      "\n",
      " 0: ember classifies as benign in orig malware set\n",
      " 699: ember classifies as benign in adversarial malware set\n",
      " 699, 12.95%: evasive samples and ratio against ember \n",
      "\n",
      " 5: ember_emulation classifies as benign in orig malware set\n",
      " 236: ember_emulation classifies as benign in adversarial malware set\n",
      " 231, 4.28%: evasive samples and ratio against ember_emulation \n",
      "\n",
      " 61: ember_filepaths_emulation classifies as benign in orig malware set\n",
      " 80: ember_filepaths_emulation classifies as benign in adversarial malware set\n",
      " 19, 0.35%: evasive samples and ratio against ember_filepaths_emulation \n",
      "\n",
      " 61: all classifies as benign in orig malware set\n",
      " 80: all classifies as benign in adversarial malware set\n",
      " 19, 0.35%: evasive samples and ratio against all \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" {((y_ember_orig_int).astype(int) == 0).sum()}: ember (secml) classifies as benign in orig malware set\")\n",
    "print(f\" {((y_ember_adv_int).astype(int) == 0).sum()}: ember (secml) classifies as benign in adversarial malware set\")\n",
    "evasive = ((y_ember_adv_int).astype(int) == 0).sum() - ((y_ember_orig_int).astype(int) == 0).sum()\n",
    "evasive_ratio = evasive*100/len(y_ember_adv_int)\n",
    "print(f\" {evasive}, {evasive_ratio:.2f}%: evasive samples and ratio against ember\")\n",
    "print()\n",
    "\n",
    "for model in models:\n",
    "    orig_benign = ((probbs[model] > 0.5).astype(int) == 0).sum()\n",
    "    adv_benign = ((probbs_adv[model] > 0.5).astype(int) == 0).sum()\n",
    "    evasive = adv_benign - orig_benign\n",
    "    evasive_ratio = evasive*100/len(probbs[model])\n",
    "    print(f\" {orig_benign}: {model} classifies as benign in orig malware set\")\n",
    "    print(f\" {adv_benign}: {model} classifies as benign in adversarial malware set\")\n",
    "    print(f\" {evasive}, {evasive_ratio:.2f}%: evasive samples and ratio against {model} \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves (DRAFT, might no be needed)\n",
    "\n",
    "Possible only if - ROC evaluation if forming a validation set with benign labels, but replacing original malware with adversarial samples if they were acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulelist = [[\"malconv\"], [\"ember\"], [\"emulation\"], [\"malconv\", \"ember\", \"filepaths\", \"emulation\"]]\n",
    "mmodels = {}\n",
    "for x in modulelist:\n",
    "    key = \"_\".join(x)\n",
    "    if key == \"malconv_ember_filepaths_emulation\":\n",
    "        key = \"All\"\n",
    "    mmodels[key] = models[key]\n",
    "\n",
    "def evaluate_adversarial_robustness(models, x_tests, x_tests_adv, y_test, y_test_adv, ax=None):\n",
    "    probs = {}\n",
    "    probs_adv = {}\n",
    "\n",
    "    model = \"No Skill\"\n",
    "    most_common_label = np.argmax(np.bincount(y_test.astype(int)))\n",
    "    probs[model] = np.array([most_common_label for _ in range(len(y_test))], dtype=int)\n",
    "    probs_adv[model] = np.array([most_common_label for _ in range(len(y_test_adv))], dtype=int)\n",
    "    \n",
    "    _, ax = plt.subplots(2, 2, figsize=(14,12))\n",
    "    ax_idx = {0: [0,0], 1: [0,1], 2: [1,0], 3:[1,1], 4:[2,0], 5:[2,1]}\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        i1, i2 = ax_idx[i][0],ax_idx[i][1]\n",
    "        probs[model] = models[model].predict_proba(x_tests[model])[:,1]\n",
    "        probs_adv[model] = models[model].predict_proba(x_tests_adv[model])[:,1]\n",
    "        # preds = np.where(probs[model] > 0.5, 1, 0)\n",
    "        # preds_adv = np.where(probs_adv[model] > 0.5, 1, 0)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, probs[model])\n",
    "        fpr_adv, tpr_adv, _ = roc_curve(y_test_adv, probs_adv[model])\n",
    "        # plot the roc curve for the model\n",
    "        linestyle = \"--\" if model == \"No Skill\" else \"solid\"\n",
    "        ax[i1,i2].plot(fpr, tpr, linestyle=linestyle, label=model)\n",
    "        ax[i1,i2].plot(fpr_adv, tpr_adv, linestyle=linestyle, label=model)\n",
    "        # axis labels\n",
    "        ax[i1,i2].set_xlabel('False Positive Rate')\n",
    "        ax[i1,i2].set_ylabel('True Positive Rate')\n",
    "        ax[i1,i2].title.set_text(model)\n",
    "        _ = ax[i1,i2].legend([\"regular\", \"adversarial\"])\n",
    "        # TBD - set legend position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of different attack -- per number of sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
